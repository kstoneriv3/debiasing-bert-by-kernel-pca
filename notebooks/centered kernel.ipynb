{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.metrics import pairwise_kernels\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centered_kernel(X, X_index, Y, Y_index, metric=\"linear\", filter_params=True, n_jobs=None, **kwds):\n",
    "    \"\"\"Compute the group-mean-centered kernel between arrays X and Y.\n",
    "    \n",
    "    This method takes either a vector array and returns a kernel matrix. \n",
    "    For the mean centering of the kernel within each group, the index to which \n",
    "    each sample belongs to must be privided as ``X_index`` and ``Y_index``. \n",
    "    When there is a sample that does not belong to any group, its index \n",
    "    must be set ``nan``, and centering is not applied to the sample.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array [n_samples_a, n_features]\n",
    "        A feature array which is sorted accoding to the group index. \n",
    "        Note that if the index of an sample is 'np.nan', the sample must come at the last of the array.  \n",
    "    X_index: integer array [n_samples]\n",
    "        A sorted array of indices to which samples in X belongs to. When a sample does \n",
    "        not belong to any group, its index must be ``np.nan``. \n",
    "    Y: array [n_samples_b, 1 + n_features]\n",
    "        A second feature array which is sorted accoding to the group index. \n",
    "        Note that if the index of an sample is 'np.nan', the sample must come at the last of the array.  \n",
    "    Y_index: integer array [n_samples]\n",
    "        A sorted array of indices to which samples in Y belongs to. When a sample does \n",
    "        not belong to any group, its index must be ``np.nan``. \n",
    "    metric : string\n",
    "        The metric to use when calculating kernel between instances in a\n",
    "        feature array. Valid values for metric are:\n",
    "        ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
    "        'laplacian', 'sigmoid', 'cosine']\n",
    "        the metric must be one of the metrics in sklearn.pairwise.PAIRWISE_KERNEL_FUNCTIONS. \n",
    "    filter_params : boolean\n",
    "        Whether to filter invalid parameters or not.\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        The number of jobs to use for the computation. This works by breaking\n",
    "        down the pairwise matrix into n_jobs even slices and computing them in\n",
    "        parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    **kwds : optional keyword parameters\n",
    "        Any further parameters are passed directly to the kernel function.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    K : array [n_samples_a, n_samples_b]\n",
    "        A mean-centered kernel matrix K such that K_{i, j} is the kernel between the\n",
    "        ith and jth vectors of the given matrix X, if Y is None.\n",
    "        If Y is not None, then K_{i, j} is the kernel between the ith array\n",
    "        from X and the jth array from Y.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from src.kernel_pca\n",
    "    \"\"\"\n",
    "    KXY = pairwise_kernels(X, Y, metric=metric, filter_params=filter_params, n_jobs=n_jobs, **kwds)\n",
    "    DX = _get_design_matrix(X_index)\n",
    "    DY = _get_design_matrix(Y_index)\n",
    "    KXY_centered = DX @ KXY @ DY\n",
    "    return KXY_centered\n",
    "    \n",
    "def _get_design_matrix(index):\n",
    "    \"\"\"Calculate the design matrix for group-mean-centering\n",
    "    \n",
    "    Example:\n",
    "    Given ``index = [1, 1, 2, 2, np.nan]``, the design matrix becomes\n",
    "                          [[1/2, 1/2,    0,   0, 0],\n",
    "                           [1/2, 1/2,    0,   0, 0],\n",
    "    np.eye(5) - np.array(  [  0,   0,  1/2, 1/2, 0],  )\n",
    "                           [  0,   0,  1/2, 1/2, 0],\n",
    "                           [  0,   0,    0,   0, 0]]\n",
    "    \"\"\"\n",
    "    N = index.shape[0]\n",
    "    mask_nan = np.isnan(index)\n",
    "    N_nan = sum(mask_nan)\n",
    "    _, group_counts = np.unique(index[~mask_nan], return_counts=True)\n",
    "    assert np.all(group_counts > 1)  # There must be more than 2 elements in each group.\n",
    "    blocks = [np.full([c, c], 1./c) for c in group_counts] + [0.0] * N_nan\n",
    "    design_matrix = scipy.sparse.eye(N) - scipy.sparse.block_diag(blocks)\n",
    "    return design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_centered_kernel():\n",
    "    \n",
    "    D = 500\n",
    "    N = 1000\n",
    "    N_grouped = 900\n",
    "    N_nan = N - N_grouped\n",
    "    num_groups = 100\n",
    "    \n",
    "    # X contains groups of various size and non-centered samples\n",
    "    X = np.random.randn(N * D).reshape(N, D)\n",
    "    X_index = np.concatenate([\n",
    "        np.repeat(np.arange(num_groups), 2),  \n",
    "        np.random.randint(0, num_groups, N_grouped - 2*num_groups),\n",
    "        np.full([N_nan], np.nan)\n",
    "    ])\n",
    "    X_index.sort()\n",
    "    \n",
    "    \n",
    "    # Y only contains non-centered samples\n",
    "    Y = np.random.randn(N * D).reshape(N, D)\n",
    "    Y_index = np.array([np.nan] * N)\n",
    "    \n",
    "    Kxx = centered_kernel(X, X_index, X, X_index, metric=\"rbf\")\n",
    "    assert(np.allclose(Kxx, Kxx.T))\n",
    "    eigvals = np.linalg.eigvalsh(Kxx)\n",
    "    assert(np.all(-1e-8 < eigvals))  # Kernel matrix must be positive definite.\n",
    "    assert(np.sum(1e-8 < eigvals) == N - num_groups)  # The effective rank of the matrix should be (N - num_groups).\n",
    "    \n",
    "    Kxy = centered_kernel(X, X_index, Y, Y_index, metric=\"rbf\")\n",
    "    Kyx = centered_kernel(Y, Y_index, X, X_index, metric=\"rbf\")\n",
    "    assert(np.allclose(Kxy, Kyx.T))  # Kernel matrix must be symmetric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_centered_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  1],\n",
       "       [ 1,  2,  3],\n",
       "       [ 2,  4,  5],\n",
       "       [ 3,  6,  7],\n",
       "       [ 4,  8,  9],\n",
       "       [ 5, 10, 11],\n",
       "       [ 6, 12, 13],\n",
       "       [ 7, 14, 15],\n",
       "       [ 8, 16, 17],\n",
       "       [ 9, 18, 19]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([\n",
    "    np.arange(10).reshape(10, 1), \n",
    "    np.arange(20).reshape(10, 2)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredKernelPCA:\n",
    "    \"\"\"Kernel PCA with mean-centering in feature space within each group.\n",
    "    \n",
    "    Remark\n",
    "    ------\n",
    "    The design of this class is based on the sklearn.decomposition.KernelPCA.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=None, *, kernel=\"linear\",\n",
    "                 gamma=None, degree=3, coef0=1, kernel_params=None,\n",
    "                 alpha=1.0, eigen_solver='auto',\n",
    "                 tol=0, max_iter=None, remove_zero_eig=False,\n",
    "                 random_state=None, copy_X=True, n_jobs=None):\n",
    "        self.n_components = n_components\n",
    "        self.kernel = kernel\n",
    "        self.kernel_params = kernel_params\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.alpha = alpha\n",
    "        self.eigen_solver = eigen_solver\n",
    "        self.remove_zero_eig = remove_zero_eig\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.copy_X = copy_X\n",
    "        self.dual_coef_ = None\n",
    "        assert callable(self.kernel)\n",
    "\n",
    "    def _get_kernel(self, X, X_index, Y, Y_index):\n",
    "        params = self.kernel_params or {}\n",
    "        return pairwise_kernels(X, X_index, Y, Y_index, metric=self.kernel,\n",
    "                                filter_params=True, n_jobs=self.n_jobs,\n",
    "                                **params)\n",
    "\n",
    "    def _fit_transform(self, K):\n",
    "        \"\"\" Fit's using kernel K\"\"\"\n",
    "        # center kernel\n",
    "        K = self._centerer.fit_transform(K)\n",
    "\n",
    "        if self.n_components is None:\n",
    "            n_components = K.shape[0]\n",
    "        else:\n",
    "            n_components = min(K.shape[0], self.n_components)\n",
    "\n",
    "        # compute eigenvectors\n",
    "        if self.eigen_solver == 'auto':\n",
    "            if K.shape[0] > 200 and n_components < 10:\n",
    "                eigen_solver = 'arpack'\n",
    "            else:\n",
    "                eigen_solver = 'dense'\n",
    "        else:\n",
    "            eigen_solver = self.eigen_solver\n",
    "\n",
    "        if eigen_solver == 'dense':\n",
    "            self.lambdas_, self.alphas_ = linalg.eigh(\n",
    "                K, eigvals=(K.shape[0] - n_components, K.shape[0] - 1))\n",
    "        elif eigen_solver == 'arpack':\n",
    "            random_state = check_random_state(self.random_state)\n",
    "            # initialize with [-1,1] as in ARPACK\n",
    "            v0 = random_state.uniform(-1, 1, K.shape[0])\n",
    "            self.lambdas_, self.alphas_ = eigsh(K, n_components,\n",
    "                                                which=\"LA\",\n",
    "                                                tol=self.tol,\n",
    "                                                maxiter=self.max_iter,\n",
    "                                                v0=v0)\n",
    "\n",
    "        # make sure that the eigenvalues are ok and fix numerical issues\n",
    "        self.lambdas_ = _check_psd_eigenvalues(self.lambdas_,\n",
    "                                               enable_warnings=False)\n",
    "\n",
    "        # flip eigenvectors' sign to enforce deterministic output\n",
    "        self.alphas_, _ = svd_flip(self.alphas_,\n",
    "                                   np.zeros_like(self.alphas_).T)\n",
    "\n",
    "        # sort eigenvectors in descending order\n",
    "        indices = self.lambdas_.argsort()[::-1]\n",
    "        self.lambdas_ = self.lambdas_[indices]\n",
    "        self.alphas_ = self.alphas_[:, indices]\n",
    "\n",
    "        # remove eigenvectors with a zero eigenvalue (null space) if required\n",
    "        if self.remove_zero_eig or self.n_components is None:\n",
    "            self.alphas_ = self.alphas_[:, self.lambdas_ > 0]\n",
    "            self.lambdas_ = self.lambdas_[self.lambdas_ > 0]\n",
    "\n",
    "        # Maintenance note on Eigenvectors normalization\n",
    "        # ----------------------------------------------\n",
    "        # there is a link between\n",
    "        # the eigenvectors of K=Phi(X)'Phi(X) and the ones of Phi(X)Phi(X)'\n",
    "        # if v is an eigenvector of K\n",
    "        #     then Phi(X)v  is an eigenvector of Phi(X)Phi(X)'\n",
    "        # if u is an eigenvector of Phi(X)Phi(X)'\n",
    "        #     then Phi(X)'u is an eigenvector of Phi(X)'Phi(X)\n",
    "        #\n",
    "        # At this stage our self.alphas_ (the v) have norm 1, we need to scale\n",
    "        # them so that eigenvectors in kernel feature space (the u) have norm=1\n",
    "        # instead\n",
    "        #\n",
    "        # We COULD scale them here:\n",
    "        #       self.alphas_ = self.alphas_ / np.sqrt(self.lambdas_)\n",
    "        #\n",
    "        # But choose to perform that LATER when needed, in `fit()` and in\n",
    "        # `transform()`.\n",
    "\n",
    "        return K\n",
    "\n",
    "    def fit_inverse_transform(self, X):\n",
    "        if hasattr(X, \"tocsr\"):\n",
    "            raise NotImplementedError(\"Inverse transform not implemented for \"\n",
    "                                      \"sparse matrices!\")\n",
    "\n",
    "        n_samples = X_transformed.shape[0]\n",
    "        K = self._get_kernel(X_transformed)\n",
    "        K.flat[::n_samples + 1] += self.alpha\n",
    "        self.dual_coef_ = linalg.solve(K, X, sym_pos=True, overwrite_a=True)\n",
    "        self.X_transformed_fit_ = X_transformed\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model from data in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples in the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        X = self._validate_data(X, accept_sparse='csr', copy=self.copy_X)\n",
    "        self._centerer = KernelCenterer()\n",
    "        K = self._get_kernel(X)\n",
    "        self._fit_transform(K)\n",
    "\n",
    "        if self.fit_inverse_transform:\n",
    "            # no need to use the kernel to transform X, use shortcut expression\n",
    "            X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n",
    "\n",
    "            self._fit_inverse_transform(X_transformed, X)\n",
    "\n",
    "        self.X_fit_ = X\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None, **params):\n",
    "        \"\"\"Fit the model from data in X and transform X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training vector, where n_samples in the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        self.fit(X, **params)\n",
    "\n",
    "        # no need to use the kernel to transform X, use shortcut expression\n",
    "        X_transformed = self.alphas_ * np.sqrt(self.lambdas_)\n",
    "\n",
    "        if self.fit_inverse_transform:\n",
    "            self._fit_inverse_transform(X_transformed, X)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Compute centered gram matrix between X and training data X_fit_\n",
    "        K = self._centerer.transform(self._get_kernel(X, self.X_fit_))\n",
    "\n",
    "        # scale eigenvectors (properly account for null-space for dot product)\n",
    "        non_zeros = np.flatnonzero(self.lambdas_)\n",
    "        scaled_alphas = np.zeros_like(self.alphas_)\n",
    "        scaled_alphas[:, non_zeros] = (self.alphas_[:, non_zeros]\n",
    "                                       / np.sqrt(self.lambdas_[non_zeros]))\n",
    "\n",
    "        # Project with a scalar product between K and the scaled eigenvectors\n",
    "        return np.dot(K, scaled_alphas)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Transform X back to original space.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_components)\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_features)\n",
    "        References\n",
    "        ----------\n",
    "        \"Learning to Find Pre-Images\", G BakIr et al, 2004.\n",
    "        \"\"\"\n",
    "        if not self.dual_coef_:\n",
    "            raise NotFittedError(\"The fit_inverse_transform was not called.\"\n",
    "                                 \" set to True when instantiating and hence \"\n",
    "                                 \"the inverse transform is not available.\")\n",
    "\n",
    "        K = self._get_kernel(X, self.X_transformed_fit_)\n",
    "        n_samples = self.X_transformed_fit_.shape[0]\n",
    "        K.flat[::n_samples + 1] += self.alpha\n",
    "        return np.dot(K, self.dual_coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from src.kernel_pca import centered_kernel\n",
    "N = 100\n",
    "D = 10\n",
    "X = np.random.randn(N * D).reshape(N, D)\n",
    "X_index = np.repeat(np.arange(N//2), 2)\n",
    "X_combined = np.concatenate([X_index.reshape(N, 1), X], axis=1)\n",
    "\n",
    "\n",
    "Y = np.random.randn(N * D).reshape(N, D)\n",
    "Y_index = np.repeat(np.arange(N//2), 2)\n",
    "Y_combined = np.concatenate([X_index.reshape(N, 1), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = lambda x, y: pairwise_kernels(x, y, metric=\"rbf\")\n",
    "D = 500\n",
    "N = 1000\n",
    "N_grouped = 900\n",
    "N_nan = N - N_grouped\n",
    "num_groups = 100\n",
    "\n",
    "# X contains groups of various size and non-centered samples\n",
    "X = np.random.randn(N * D).reshape(N, D)\n",
    "X_index = np.concatenate([\n",
    "    np.repeat(np.arange(num_groups), 2),  \n",
    "    np.random.randint(0, num_groups, N_grouped - 2*num_groups),\n",
    "    np.full([N_nan], np.nan)\n",
    "])\n",
    "X_index.sort()\n",
    "\n",
    "\n",
    "# Y only contains non-centered samples\n",
    "Y = np.random.randn(N * D).reshape(N, D)\n",
    "Y_index = np.array([np.nan] * N)\n",
    "\n",
    "Kxx = centered_kernel(X, X_index, X, X_index, kernel)\n",
    "assert(np.allclose(Kxx, Kxx.T))  # Kernel matrix must be symmetric. \n",
    "eigvals = np.linalg.eigvalsh(Kxx)\n",
    "assert(np.all(-1e-8 < eigvals))  # Kernel matrix must be positive definite.\n",
    "assert(np.sum(1e-8 < eigvals) == N - num_groups)  # The effective rank of the matrix should be (N - num_groups).\n",
    "\n",
    "Kxy = centered_kernel(X, X_index, Y, Y_index, kernel)\n",
    "Kyx = centered_kernel(Y, Y_index, X, X_index, kernel)\n",
    "assert(np.allclose(Kxy, Kyx.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.eye(10)\n",
    "A.flat[::10+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
